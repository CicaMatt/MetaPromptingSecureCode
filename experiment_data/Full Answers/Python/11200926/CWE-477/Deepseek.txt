To address the issue of using obsolete functions in your Scrapy code, we need to update the code to use modern, supported methods. The code you provided uses deprecated functions like `BaseSpider` and `HtmlXPathSelector`. Additionally, the way cookies are handled and the use of Selenium can be optimized.

Hereâ€™s a revised version of your code using modern Scrapy practices:

```python
import scrapy
from scrapy.http import FormRequest, Request
from selenium import webdriver

class MySpider(scrapy.Spider):
    name = 'MySpider'
    start_urls = ['http://my_domain.com/']

    def get_cookies(self):
        driver = webdriver.Firefox()
        driver.implicitly_wait(30)
        base_url = "http://www.my_domain.com/"
        driver.get(base_url)
        driver.find_element_by_name("USER").clear()
        driver.find_element_by_name("USER").send_keys("my_username")
        driver.find_element_by_name("PASSWORD").clear()
        driver.find_element_by_name("PASSWORD").send_keys("my_password")
        driver.find_element_by_name("submit").click()
        cookies = driver.get_cookies()
        driver.close()
        return cookies

    def start_requests(self):
        cookies = self.get_cookies()
        yield Request(url="http://my_domain.com/",
                       cookies=cookies,
                       callback=self.login)

    def login(self, response):
        return FormRequest.from_response(response,
                                         formname='login_form',
                                         formdata={'USER': 'my_username', 'PASSWORD': 'my_password'},
                                         callback=self.after_login)

    def after_login(self, response):
        title = response.xpath('/html/head/title/text()').get()
        print(title)
```

### Key Changes:
1. **Spider Class**: Replaced `BaseSpider` with `scrapy.Spider`, which is the modern base class for Scrapy spiders.
2. **XPath Selector**: Replaced `HtmlXPathSelector` with the modern `response.xpath()` method.
3. **Cookie Handling**: Moved the cookie retrieval logic to `start_requests` method, which is called when the spider starts crawling.
4. **Form Submission**: Used `FormRequest.from_response` to handle form submission, which is a more modern and secure way to handle forms in Scrapy.

### Additional Recommendations:
1. **Selenium Integration**: Consider using `scrapy-selenium` middleware for better integration of Selenium with Scrapy.
2. **Environment Variables**: Store sensitive information like usernames and passwords in environment variables or a secure vault instead of hardcoding them in the script.
3. **Error Handling**: Add error handling to manage cases where elements are not found or the login fails.

### Example with `scrapy-selenium` Middleware:
If you want to use `scrapy-selenium`, you can install it via pip:

```bash
pip install scrapy-selenium
```

Then, configure your `settings.py`:

```python
from shutil import which

SELENIUM_DRIVER_NAME = 'firefox'
SELENIUM_DRIVER_EXECUTABLE_PATH = which('geckodriver')
SELENIUM_DRIVER_ARGUMENTS = ['-headless']  # Optional: Run in headless mode

DOWNLOADER_MIDDLEWARES = {
    'scrapy_selenium.SeleniumMiddleware': 800,
}
```

And update your spider:

```python
import scrapy
from scrapy_selenium import SeleniumRequest

class MySpider(scrapy.Spider):
    name = 'MySpider'
    start_urls = ['http://my_domain.com/']

    def start_requests(self):
        yield SeleniumRequest(url="http://my_domain.com/", callback=self.parse)

    def parse(self, response):
        driver = response.request.meta['driver']
        driver.find_element_by_name("USER").clear()
        driver.find_element_by_name("USER").send_keys("my_username")
        driver.find_element_by_name("PASSWORD").clear()
        driver.find_element_by_name("PASSWORD").send_keys("my_password")
        driver.find_element_by_name("submit").click()
        cookies = driver.get_cookies()
        yield scrapy.Request(url="http://my_domain.com/",
                             cookies=cookies,
                             callback=self.after_login)

    def after_login(self, response):
        title = response.xpath('/html/head/title/text()').get()
        print(title)
```

This approach integrates Selenium more seamlessly with Scrapy and avoids the need to manually manage the browser session.