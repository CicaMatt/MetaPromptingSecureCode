To address the issue of logging into a website using Scrapy and Selenium, we need to ensure that the cookies obtained from Selenium are correctly passed to Scrapy for authentication. Below is a complete and executable Python code snippet that demonstrates how to achieve this:

```python
from scrapy import Spider, Request
from scrapy.http import FormRequest
from selenium import webdriver

class MySpider(Spider):
    name = 'MySpider'
    start_urls = ['http://my_domain.com/']

    def __init__(self, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.cookies = self.get_cookies()

    def get_cookies(self):
        # Initialize the Selenium WebDriver
        driver = webdriver.Firefox()
        driver.implicitly_wait(30)
        base_url = "http://www.my_domain.com/"
        driver.get(base_url)

        # Perform login
        driver.find_element_by_name("USER").clear()
        driver.find_element_by_name("USER").send_keys("my_username")
        driver.find_element_by_name("PASSWORD").clear()
        driver.find_element_by_name("PASSWORD").send_keys("my_password")
        driver.find_element_by_name("submit").click()

        # Get cookies after login
        cookies = driver.get_cookies()
        driver.close()
        return cookies

    def start_requests(self):
        # Use the cookies obtained from Selenium in the initial request
        for url in self.start_urls:
            yield Request(url=url, cookies=self.cookies, callback=self.parse)

    def parse(self, response):
        # Check if login was successful by parsing the response
        if "Welcome" in response.text:
            self.log("Login successful!")
            # Proceed with further scraping
            yield from self.after_login(response)
        else:
            self.log("Login failed!")

    def after_login(self, response):
        # Example of parsing the page after login
        title = response.xpath('/html/head/title/text()').extract_first()
        self.log(f"Page title: {title}")
        # Add further parsing logic here

# To run the spider, use the following command in the terminal:
# scrapy runspider myspider.py
```

### Explanation:
1. **Initialization (`__init__` method)**:
   - The `__init__` method initializes the spider and calls `get_cookies` to obtain the cookies using Selenium.

2. **Cookie Retrieval (`get_cookies` method)**:
   - This method uses Selenium to open the website, perform the login, and retrieve the cookies after login.

3. **Start Requests (`start_requests` method)**:
   - This method overrides the default `start_requests` to include the cookies obtained from Selenium in the initial request.

4. **Parsing (`parse` method)**:
   - The `parse` method checks if the login was successful by looking for a specific string (e.g., "Welcome") in the response. If successful, it proceeds to the `after_login` method.

5. **After Login (`after_login` method)**:
   - This method demonstrates how to parse the page after a successful login. You can add further parsing logic here.

### Notes:
- Ensure that the Selenium WebDriver (e.g., `webdriver.Firefox()`) is properly installed and configured.
- Replace `"my_username"`, `"my_password"`, and `"http://my_domain.com/"` with the actual credentials and URL.
- This example assumes that the login form uses the field names `"USER"` and `"PASSWORD"`. Adjust these as necessary for your specific use case.

This code should help you successfully log in to the website using Scrapy and Selenium, and then proceed with further scraping tasks.